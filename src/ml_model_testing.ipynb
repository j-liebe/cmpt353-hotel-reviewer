{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0da141d",
   "metadata": {},
   "source": [
    "### This script is written to test our Naive Bayes, Random Forest Classifier and Linear SVC model to easier testing\n",
    "\n",
    "#### We have used non-San Francisco reviews are training and testing dataset and San Francisco Hotel reviews as prediction dataset\n",
    "\n",
    "#### These working model were then adapted into machine_learning.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4527140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql import SparkSession, functions, types\n",
    "from pyspark.sql.functions import col, when, coalesce\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f47ba4",
   "metadata": {},
   "source": [
    "### Use Spark to read in training and prediction json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "465b9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Spark stuff\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Machine Learning Models\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config('spark.driver.memory', '8g') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .config('spark.network.timeout', '600000') \\\n",
    "    .config('spark.sql.broadcastTimeout', '600000') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "328273eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for review_comments.json\n",
    "review_comments_schema = types.StructType([\n",
    "    types.StructField('comment_id', types.StringType()),\n",
    "    types.StructField('review', types.StringType()),\n",
    "    types.StructField('review_text', types.StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "265b86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_spark_df = spark.read.load(\"./../filtered_data/training_data.json\", format=\"json\", schema = review_comments_schema).cache()\n",
    "prediction_spark_df = spark.read.load(\"./../filtered_data/validation_data.json\", format=\"json\", schema = review_comments_schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6273e998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training_spark_df after reading : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131760, 3)\n",
      "Dimensions of prediction_spark_df after reading : \n",
      "(15274, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of training_spark_df after reading : \")\n",
    "print((training_spark_df.count(), len(training_spark_df.columns)))\n",
    "\n",
    "print(\"Dimensions of prediction_spark_df after reading : \")\n",
    "print((prediction_spark_df.count(), len(prediction_spark_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c4e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a102b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "744968f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null from all the columns before running the model\n",
    "training_spark_df = training_spark_df.na.drop()\n",
    "\n",
    "prediction_spark_df = prediction_spark_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "884b8eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training_spark_df after removing null values : \n",
      "(131673, 3)\n",
      "Dimensions of prediction_spark_df after removing null values  : \n",
      "(15261, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of training_spark_df after removing null values : \")\n",
    "print((training_spark_df.count(), len(training_spark_df.columns)))\n",
    "\n",
    "print(\"Dimensions of prediction_spark_df after removing null values  : \")\n",
    "print((prediction_spark_df.count(), len(prediction_spark_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47fc32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing\n",
    "negative_training_reviews = training_spark_df.filter(training_spark_df.review == \"negative\")\n",
    "positive_training_reviews = training_spark_df.filter(training_spark_df.review == \"positive\")\n",
    "\n",
    "negative_training_reviews_counts = negative_training_reviews.count()\n",
    "original_training_data_count = training_spark_df.count()\n",
    "sampled_positive_training_reviews = positive_training_reviews.sample((negative_training_reviews_counts/original_training_data_count) + 0.1)\n",
    "\n",
    "balanced_training_data_spark_df = sampled_positive_training_reviews.union(negative_training_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c0c3269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of balanced_training_data_spark_df: \n",
      "(45691, 3)\n",
      "Dimensions of prediction_spark_df: \n",
      "(15261, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of balanced_training_data_spark_df: \")\n",
    "print((balanced_training_data_spark_df.count(), len(balanced_training_data_spark_df.columns)))\n",
    "\n",
    "print(\"Dimensions of prediction_spark_df: \")\n",
    "print((prediction_spark_df.count(), len(prediction_spark_df.columns)))\n",
    "\n",
    "# Notice dimensions of training data is reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7666e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split = balanced_training_data_spark_df.randomSplit(weights = [0.80, 0.20], seed = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d0693",
   "metadata": {},
   "source": [
    "### Train Multinomial Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87326c",
   "metadata": {},
   "source": [
    "Taken help to write NLP pipeline from this article https://medium.datadriveninvestor.com/nlp-with-pyspark-9e5f1fca7adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "95d7b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:39:56 WARN DAGScheduler: Broadcasting large task binary with size 1234.2 KiB\n",
      "23/08/02 22:39:58 WARN DAGScheduler: Broadcasting large task binary with size 1217.2 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# feature engineering on review_text for NLP\n",
    "tokenizer = Tokenizer(inputCol = \"review_text\", \n",
    "                      outputCol = \"words\")\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol = \"words\", \n",
    "                                    outputCol = \"words_without_stopwords\")\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol = \"words_without_stopwords\", \n",
    "                                  outputCol = \"features\")\n",
    "\n",
    "# We have strings(poitive or negative) in reviews which are our expected converting to index\n",
    "labelEncoder = StringIndexer(inputCol = 'review', \n",
    "                             outputCol = 'reviewIndexed').fit(train_split)\n",
    "\n",
    "# Use NaiveBayes multinomial model on features and reviewIndexed \n",
    "naive_bayes = NaiveBayes(modelType = \"multinomial\", \n",
    "                         featuresCol = 'features', \n",
    "                         labelCol = 'reviewIndexed')\n",
    "\n",
    "# Convert prediction which are indexed format to labels\n",
    "labelConverter = IndexToString(inputCol = \"prediction\", \n",
    "                               outputCol = \"predicted_review_label\", \n",
    "                               labels = labelEncoder.labels)\n",
    "\n",
    "# make pipeline\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        tokenizer,\n",
    "        stopWordsRemover, \n",
    "        vectorizer,\n",
    "        labelEncoder, \n",
    "        naive_bayes, \n",
    "        labelConverter\n",
    "    ])\n",
    "\n",
    "# fit training data which is non-ny hotel reviews\n",
    "naive_bayes_model = pipeline.fit(train_split)\n",
    "\n",
    "# make predictions on validating data which is ny-hotel reviews\n",
    "nb_predictions = naive_bayes_model.transform(prediction_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cdba5ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:39:58 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "[Stage 282:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NaiveBayes on predictions data is = 0.931001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# score on validations data set / predictions\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"reviewIndexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
    "print(\"Accuracy of NaiveBayes on predictions data is = %g\"% (nb_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c9aee82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:39:59 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/08/02 22:40:00 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "[Stage 285:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9310005897385493\n",
      "Precision :  0.9460645737241482\n",
      "f1Score :  0.9602446483180428\n",
      "recall :  0.9748562667688769\n",
      "false poitive rate :  0.32716606498194944\n",
      "true poitive rate :  0.9748562667688769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nd_metric = MulticlassMetrics(nb_predictions['reviewIndexed','prediction'].rdd)\n",
    "\n",
    "print(\"Accuracy : \", nd_metric.accuracy)\n",
    "print(\"Precision : \", nd_metric.precision(0.0))\n",
    "print(\"f1Score : \", nd_metric.fMeasure(0.0))\n",
    "print(\"recall : \", nd_metric.recall(0.0))\n",
    "print(\"false poitive rate : \", nd_metric.falsePositiveRate(0.0))\n",
    "print(\"true poitive rate : \", nd_metric.truePositiveRate(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e3cf578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:40:01 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NaiveBayes on test dataset = 0.907288\n"
     ]
    }
   ],
   "source": [
    "nb_test_split_predictions = naive_bayes_model.transform(test_split)\n",
    "# score on validations data set / predictions\n",
    "nb_training_evaluator = MulticlassClassificationEvaluator(labelCol=\"reviewIndexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nb_training_accuracy = nb_training_evaluator.evaluate(nb_test_split_predictions)\n",
    "print(\"Accuracy of NaiveBayes on test dataset = %g\"% (nb_training_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e77f747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:40:02 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/08/02 22:40:02 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "[Stage 290:=========================================>             (24 + 8) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9072884983665653\n",
      "Precision :  0.9293488638533511\n",
      "f1Score :  0.9220422468504309\n",
      "recall :  0.9148496240601504\n",
      "false poitive rate :  0.10402024177677818\n",
      "true poitive rate :  0.9148496240601504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 290:===================================================>   (30 + 2) / 32]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nd_test_split_metric = MulticlassMetrics(nb_test_split_predictions['reviewIndexed','prediction'].rdd)\n",
    "\n",
    "print(\"Accuracy : \", nd_test_split_metric.accuracy)\n",
    "print(\"Precision : \", nd_test_split_metric.precision(0.0))\n",
    "print(\"f1Score : \", nd_test_split_metric.fMeasure(0.0))\n",
    "print(\"recall : \", nd_test_split_metric.recall(0.0))\n",
    "print(\"false poitive rate : \", nd_test_split_metric.falsePositiveRate(0.0))\n",
    "print(\"true poitive rate : \", nd_test_split_metric.truePositiveRate(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5df43ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:40:04 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "[Stage 292:===================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NaiveBayes on training dataset = 0.928397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 292:=====================================================> (31 + 1) / 32]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nb_train_split_predictions = naive_bayes_model.transform(train_split)\n",
    "# score on validations data set / predictions\n",
    "nb_training_evaluator = MulticlassClassificationEvaluator(labelCol=\"reviewIndexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nb_training_accuracy = nb_training_evaluator.evaluate(nb_train_split_predictions)\n",
    "print(\"Accuracy of NaiveBayes on training dataset = %g\"% (nb_training_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "788c3655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:40:06 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/08/02 22:40:06 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "[Stage 295:===================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.928396805563101\n",
      "Precision :  0.9385492700729927\n",
      "f1Score :  0.939792608834681\n",
      "recall :  0.9410392461805873\n",
      "false poitive rate :  0.09008828250401284\n",
      "true poitive rate :  0.9410392461805873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 295:=====================================================> (31 + 1) / 32]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nd_train_split_metric = MulticlassMetrics(nb_train_split_predictions['reviewIndexed','prediction'].rdd)\n",
    "\n",
    "print(\"Accuracy : \", nd_train_split_metric.accuracy)\n",
    "print(\"Precision : \", nd_train_split_metric.precision(0.0))\n",
    "print(\"f1Score : \", nd_train_split_metric.fMeasure(0.0))\n",
    "print(\"recall : \", nd_train_split_metric.recall(0.0))\n",
    "print(\"false poitive rate : \", nd_train_split_metric.falsePositiveRate(0.0))\n",
    "print(\"true poitive rate : \", nd_train_split_metric.truePositiveRate(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "093efb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_prediction_spark_df = nb_predictions.select(\"comment_id\",\n",
    "#                                             \"review\",\n",
    "#                                             \"review_text\",\n",
    "#                                             \"reviewIndexed\",\n",
    "#                                             \"prediction\",\n",
    "#                                             \"predicted_review_label\"\n",
    "#                                            )\n",
    "# # nb_prediction_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "47d6de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # converting results to pandas and merge with original ny-hotel-english-data\n",
    "# nb_prediction_pandas_df = nb_prediction_spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e7f0e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_prediction_pandas_df['comment_id'] = nb_prediction_pandas_df['comment_id'].astype('int64')\n",
    "# nb_prediction_on_ny_hotel_english_reviews = validating_ny_hotel_pd.merge(nb_prediction_pandas_df, on = 'comment_id', how = 'inner')\n",
    "# nb_prediction_on_ny_hotel_english_reviews = nb_prediction_on_ny_hotel_english_reviews.drop(\"review_text\", axis = 1)\n",
    "\n",
    "# # nb_prediction_on_ny_hotel_english_reviews\n",
    "# # 40007 rows × 26 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d712ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_prediction_on_ny_hotel_english_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a1cb980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results as csv\n",
    "#nb_prediction_pandas_df.to_csv('./../predicted_data/nb_prediction_on_ny_hotel_english_reviews.csv.gz', index = False, compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49912c09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86c7a9bd",
   "metadata": {},
   "source": [
    "### Train Random Forest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering on review_text for NLP\n",
    "tokenizer = Tokenizer(inputCol = \"review_text\", \n",
    "                      outputCol = \"words\")\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol = \"words\", \n",
    "                                    outputCol = \"words_without_stopwords\")\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol = \"words_without_stopwords\", \n",
    "                                  outputCol = \"features\")\n",
    "\n",
    "# We have strings(poitive or negative) in reviews which are our expected converting to index\n",
    "labelEncoder = StringIndexer(inputCol = 'review', \n",
    "                             outputCol = 'reviewIndexed').fit(train_split)\n",
    "\n",
    "# Use NaiveBayes multinomial model on features and reviewIndexed \n",
    "random_forest = RandomForestClassifier(numTrees=100, maxDepth=30, featuresCol = 'features', \n",
    "                         labelCol = 'reviewIndexed')\n",
    "\n",
    "# Convert prediction which are indexed format to labels\n",
    "labelConverter = IndexToString(inputCol = \"prediction\", \n",
    "                               outputCol = \"predicted_review_label\", \n",
    "                               labels = labelEncoder.labels)\n",
    "\n",
    "# make pipeline\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        tokenizer,\n",
    "        stopWordsRemover, \n",
    "        vectorizer,\n",
    "        labelEncoder, \n",
    "        random_forest, \n",
    "        labelConverter\n",
    "    ])\n",
    "\n",
    "# fit training data which is non-ny hotel reviews\n",
    "random_forest_classifier_model = pipeline.fit(train_split)\n",
    "\n",
    "# make predictions on validating data which is ny-hotel reviews\n",
    "rfc_predictions = random_forest_classifier_model.transform(prediction_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1147fcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:54:11 WARN DAGScheduler: Broadcasting large task binary with size 19.2 MiB\n",
      "[Stage 368:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifer on prediction data = 0.926676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# score on validations data set / predictions\n",
    "rfc_evaluator = MulticlassClassificationEvaluator(labelCol=\"reviewIndexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rfc_accuracy = rfc_evaluator.evaluate(rfc_predictions)\n",
    "print(\"Accuracy of Random Forest Classifer on prediction data = %g\"% (rfc_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "034fc869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:54:16 WARN DAGScheduler: Broadcasting large task binary with size 19.2 MiB\n",
      "23/08/02 22:54:18 WARN DAGScheduler: Broadcasting large task binary with size 19.2 MiB\n",
      "[Stage 371:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9266758403774327\n",
      "Precision :  0.9764915935128701\n",
      "f1Score :  0.9591173139454167\n",
      "recall :  0.9423504917797401\n",
      "false poitive rate :  0.23723723723723725\n",
      "true poitive rate :  0.9423504917797401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rfc_metric = MulticlassMetrics(rfc_predictions['reviewIndexed','prediction'].rdd)\n",
    "\n",
    "print(\"Accuracy : \", rfc_metric.accuracy)\n",
    "print(\"Precision : \", rfc_metric.precision(0.0))\n",
    "print(\"f1Score : \", rfc_metric.fMeasure(0.0))\n",
    "print(\"recall : \", rfc_metric.recall(0.0))\n",
    "print(\"false poitive rate : \", rfc_metric.falsePositiveRate(0.0))\n",
    "print(\"true poitive rate : \", rfc_metric.truePositiveRate(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "afef39c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:54:22 WARN DAGScheduler: Broadcasting large task binary with size 19.2 MiB\n",
      "[Stage 373:===================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifer on prediction data = 0.83767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 373:=====================================================> (31 + 1) / 32]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rfc_test_split_predictions = random_forest_classifier_model.transform(test_split)\n",
    "rfc_evaluator = MulticlassClassificationEvaluator(labelCol=\"reviewIndexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rfc_accuracy = rfc_evaluator.evaluate(rfc_test_split_predictions)\n",
    "print(\"Accuracy of Random Forest Classifer on prediction data = %g\"% (rfc_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fd997b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:54:27 WARN DAGScheduler: Broadcasting large task binary with size 19.2 MiB\n",
      "23/08/02 22:54:28 WARN DAGScheduler: Broadcasting large task binary with size 19.2 MiB\n",
      "[Stage 376:===================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8376703841387856\n",
      "Precision :  0.9683024632423143\n",
      "f1Score :  0.8755935422602089\n",
      "recall :  0.799086038449417\n",
      "false poitive rate :  0.06558672461477677\n",
      "true poitive rate :  0.799086038449417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rfc_test_split_metric = MulticlassMetrics(rfc_test_split_predictions['reviewIndexed','prediction'].rdd)\n",
    "\n",
    "print(\"Accuracy : \", rfc_test_split_metric.accuracy)\n",
    "print(\"Precision : \", rfc_test_split_metric.precision(0.0))\n",
    "print(\"f1Score : \", rfc_test_split_metric.fMeasure(0.0))\n",
    "print(\"recall : \", rfc_test_split_metric.recall(0.0))\n",
    "print(\"false poitive rate : \", rfc_test_split_metric.falsePositiveRate(0.0))\n",
    "print(\"true poitive rate : \", rfc_test_split_metric.truePositiveRate(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a2d33f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:54:34 WARN DAGScheduler: Broadcasting large task binary with size 19.2 MiB\n",
      "[Stage 378:=====================================================> (31 + 1) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifer on training dataset is 0.886864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rfc_train_split_predictions = random_forest_classifier_model.transform(train_split)\n",
    "# score on validations data set / predictions\n",
    "rfc_training_evaluator = MulticlassClassificationEvaluator(labelCol=\"reviewIndexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rfc_training_accuracy = rfc_training_evaluator.evaluate(rfc_train_split_predictions)\n",
    "print(\"Accuracy of Random Forest Classifer on training dataset is %g\"% (rfc_training_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6ab605b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:54:43 WARN DAGScheduler: Broadcasting large task binary with size 19.2 MiB\n",
      "23/08/02 22:54:44 WARN DAGScheduler: Broadcasting large task binary with size 19.2 MiB\n",
      "[Stage 381:=====================================================> (31 + 1) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8868636931602107\n",
      "Precision :  0.9938868613138686\n",
      "f1Score :  0.912751115486939\n",
      "recall :  0.8438625711740326\n",
      "false poitive rate :  0.012185141402200601\n",
      "true poitive rate :  0.8438625711740326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rfc_train_split_metric = MulticlassMetrics(rfc_train_split_predictions['reviewIndexed','prediction'].rdd)\n",
    "\n",
    "print(\"Accuracy : \", rfc_train_split_metric.accuracy)\n",
    "print(\"Precision : \", rfc_train_split_metric.precision(0.0))\n",
    "print(\"f1Score : \", rfc_train_split_metric.fMeasure(0.0))\n",
    "print(\"recall : \", rfc_train_split_metric.recall(0.0))\n",
    "print(\"false poitive rate : \", rfc_train_split_metric.falsePositiveRate(0.0))\n",
    "print(\"true poitive rate : \", rfc_train_split_metric.truePositiveRate(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aad7ce",
   "metadata": {},
   "source": [
    "### Train SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b43667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering on review_text for NLP\n",
    "tokenizer = Tokenizer(inputCol = \"review_text\", \n",
    "                      outputCol = \"words\")\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol = \"words\", \n",
    "                                    outputCol = \"words_without_stopwords\")\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol = \"words_without_stopwords\", \n",
    "                                  outputCol = \"features\")\n",
    "\n",
    "# We have strings(poitive or negative) in reviews which are our expected converting to index\n",
    "labelEncoder = StringIndexer(inputCol = 'review', \n",
    "                             outputCol = 'reviewIndexed').fit(train_split)\n",
    "\n",
    "# Use NaiveBayes multinomial model on features and reviewIndexed \n",
    "svc = LinearSVC(maxIter=20,\n",
    "                regParam = 0.01,\n",
    "                featuresCol = 'features', \n",
    "                labelCol = 'reviewIndexed')\n",
    "\n",
    "# Convert prediction which are indexed format to labels\n",
    "labelConverter = IndexToString(inputCol = \"prediction\", \n",
    "                               outputCol = \"predicted_review_label\", \n",
    "                               labels = labelEncoder.labels)\n",
    "\n",
    "# make pipeline\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        tokenizer,\n",
    "        stopWordsRemover, \n",
    "        vectorizer,\n",
    "        labelEncoder, \n",
    "        svc, \n",
    "        labelConverter\n",
    "    ])\n",
    "\n",
    "# fit training data which is non-ny hotel reviews\n",
    "svc_model = pipeline.fit(train_split)\n",
    "\n",
    "# make predictions on validating data which is ny-hotel reviews\n",
    "svc_predictions = svc_model.transform(prediction_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "012e228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:55:17 WARN DAGScheduler: Broadcasting large task binary with size 1982.7 KiB\n",
      "[Stage 476:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear SVC on prediction data = 0.928314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# score on validations data set / predictions\n",
    "svc_evaluator = MulticlassClassificationEvaluator(labelCol=\"reviewIndexed\", \n",
    "                                                  predictionCol=\"prediction\", \n",
    "                                                  metricName=\"accuracy\")\n",
    "svc_accuracy = svc_evaluator.evaluate(svc_predictions)\n",
    "print(\"Accuracy of Linear SVC on prediction data = %g\"% (svc_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "25259d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:55:18 WARN DAGScheduler: Broadcasting large task binary with size 1978.6 KiB\n",
      "23/08/02 22:55:18 WARN DAGScheduler: Broadcasting large task binary with size 1990.2 KiB\n",
      "[Stage 479:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9283140030142193\n",
      "Precision :  0.9386252045826514\n",
      "f1Score :  0.9584472804618657\n",
      "recall :  0.9791246313828962\n",
      "false poitive rate :  0.3473684210526316\n",
      "true poitive rate :  0.9791246313828962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "svc_metric = MulticlassMetrics(svc_predictions['reviewIndexed','prediction'].rdd)\n",
    "\n",
    "print(\"Accuracy : \", svc_metric.accuracy)\n",
    "print(\"Precision : \", svc_metric.precision(0.0))\n",
    "print(\"f1Score : \", svc_metric.fMeasure(0.0))\n",
    "print(\"recall : \", svc_metric.recall(0.0))\n",
    "print(\"false poitive rate : \", svc_metric.falsePositiveRate(0.0))\n",
    "print(\"true poitive rate : \", svc_metric.truePositiveRate(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "15d18cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:55:20 WARN DAGScheduler: Broadcasting large task binary with size 1996.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear SVC on training data is = 0.913259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 481:===================================================>   (30 + 2) / 32]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "svc_test_split_predictions = svc_model.transform(test_split)\n",
    "# score on validations data set / predictions\n",
    "svc_training_evaluator = MulticlassClassificationEvaluator(labelCol=\"reviewIndexed\", \n",
    "                                                  predictionCol=\"prediction\", \n",
    "                                                  metricName=\"accuracy\")\n",
    "svc_training_accuracy = svc_training_evaluator.evaluate(svc_test_split_predictions)\n",
    "print(\"Accuracy of Linear SVC on training data is = %g\"% (svc_training_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e7b6b323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:55:20 WARN DAGScheduler: Broadcasting large task binary with size 1996.4 KiB\n",
      "23/08/02 22:55:21 WARN DAGScheduler: Broadcasting large task binary with size 2008.1 KiB\n",
      "[Stage 484:===================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9132589838909542\n",
      "Precision :  0.931640252052702\n",
      "f1Score :  0.9268617021276596\n",
      "recall :  0.9221319221319221\n",
      "false poitive rate :  0.09983268265476855\n",
      "true poitive rate :  0.9221319221319221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "svc_test_split_metric = MulticlassMetrics(svc_test_split_predictions['reviewIndexed','prediction'].rdd)\n",
    "\n",
    "print(\"Accuracy : \", svc_test_split_metric.accuracy)\n",
    "print(\"Precision : \", svc_test_split_metric.precision(0.0))\n",
    "print(\"f1Score : \", svc_test_split_metric.fMeasure(0.0))\n",
    "print(\"recall : \", svc_test_split_metric.recall(0.0))\n",
    "print(\"false poitive rate : \", svc_test_split_metric.falsePositiveRate(0.0))\n",
    "print(\"true poitive rate : \", svc_test_split_metric.truePositiveRate(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b170d073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:55:22 WARN DAGScheduler: Broadcasting large task binary with size 1996.7 KiB\n",
      "[Stage 486:===================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear SVC on training data is = 0.993915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 486:=====================================================> (31 + 1) / 32]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "svc_train_split_predictions = svc_model.transform(train_split)\n",
    "# score on validations data set / predictions\n",
    "svc_training_evaluator = MulticlassClassificationEvaluator(labelCol=\"reviewIndexed\", \n",
    "                                                  predictionCol=\"prediction\", \n",
    "                                                  metricName=\"accuracy\")\n",
    "svc_training_accuracy = svc_training_evaluator.evaluate(svc_train_split_predictions)\n",
    "print(\"Accuracy of Linear SVC on training data is = %g\"% (svc_training_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "670b9324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 22:55:24 WARN DAGScheduler: Broadcasting large task binary with size 1996.4 KiB\n",
      "23/08/02 22:55:24 WARN DAGScheduler: Broadcasting large task binary with size 2008.1 KiB\n",
      "[Stage 489:===================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.993915358287608\n",
      "Precision :  0.9972171532846715\n",
      "f1Score :  0.9949023713076328\n",
      "recall :  0.992598310780129\n",
      "false poitive rate :  0.004123850730124392\n",
      "true poitive rate :  0.992598310780129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "svc_train_split_metric = MulticlassMetrics(svc_train_split_predictions['reviewIndexed','prediction'].rdd)\n",
    "\n",
    "print(\"Accuracy : \", svc_train_split_metric.accuracy)\n",
    "print(\"Precision : \", svc_train_split_metric.precision(0.0))\n",
    "print(\"f1Score : \", svc_train_split_metric.fMeasure(0.0))\n",
    "print(\"recall : \", svc_train_split_metric.recall(0.0))\n",
    "print(\"false poitive rate : \", svc_train_split_metric.falsePositiveRate(0.0))\n",
    "print(\"true poitive rate : \", svc_train_split_metric.truePositiveRate(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220dc459",
   "metadata": {},
   "source": [
    "### Next step is analysis on obtained predictions to return user friendly results\n",
    "\n",
    "* Generate WordCloud\n",
    "* Perform analysis by grouping relevant data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
